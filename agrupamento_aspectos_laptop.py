# -*- coding: utf-8 -*-
"""Agrupamento_aspectos_laptop.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4IOMAJm7x1ts3qxD7keReOL9BH4D4ja

Código em Python, responsável por realizar o agrupamento semântico de uma lista de aspectos referentes a dados extraídos de review reais sobre laptops contida em um arquivo de entrada .CSV.

Importação das bibliotecas utilizadas:
"""

import numpy as np
import pandas as pd
import spacy as sp
import nltk
import re
from google.colab import drive
import string

"""Carregamento do arquivo de entrada (lista de aspectos) e formatação:"""

# Utilizando o drive do usuário: 
drive.mount("/content/drive")

with open('/content/drive/My Drive/laptop_filtered_aspect_sample.csv', mode='r', encoding='utf-8') as f:
  listaAspectos = f.read()

listaAspectos = listaAspectos.split('\n')

# Elimina o cabeçalho (item cujo índice é zero e que dá nome à coluna do arquivo 
# e não repensenta um dado de fato, isto é, não é um "aspecto")
listaAspectos.pop(0)

# Conferindo...
print(len(listaAspectos))

print(listaAspectos)

"""Separação da lista. O arquivo possui termos simples (uma única palavra) e compostos (formados por duas palavras). Para facilitar a análise e o respectivo agrupamento semântico é realizado tal separação. Além disso, há termos grafados em português na lista que é majoritariamente composta por palavras escritas em Inglês. Assim, é feita essa separação também."""

# Função que retorna duas listas (as formadas por palavras simples e as compostas):
def separaLista(lista):
  listaAuxSimples = []
  listaAuxCompostas = []
  for expressao in lista:
    if re.search(' ', expressao):
      listaAuxCompostas.append(expressao)
    else:
      listaAuxSimples.append(expressao)
  return listaAuxSimples, listaAuxCompostas

# Separa-se o arquivo em duas listas distintas:
listaAspectosSimples, listaAspectosCompostos = separaLista(listaAspectos)

# Conferindo...
print(len(listaAspectosSimples))
print(listaAspectosSimples)
print(len(listaAspectosCompostos))
print(listaAspectosCompostos)

"""Tomando a lista de aspectos formadas por palavras únicas objetiva-se agora separar as palavras grafadas em portugês das em inglês. Para a tarefa são utilizadas as bibliotecas multilingual do NLTK (Natural Language Tool Kit) e do spaCy. Elas possuem diversas ferramentas para manipulação, tratamento de bases e outros recursos junto a um extenso dicionário embutido (no caso do NLTK)."""

# Importação dos modelos necessários para a tarefa de separação:
nltk.download('wordnet')
nltk.download('omw') # Open Multilingual WordNet
nltk.download('stopwords')
from nltk.stem import PorterStemmer # usada p/ extrair radical da palavra
!python3 -m spacy download pt_core_news_sm
import pt_core_news_sm
!python3 -m spacy download en_core_web_lg
import en_core_web_lg

# Carregamento da base de corpus (é utilizada aqui a wordnet que trata-se de uma 
# grande e abrangente banco de dados léxico). Lista de línguas que podem ser
# utilizadas: ("eng", "ind", "zsm", "jpn", "tha", "cmn", "qcn", "fas", "arb",
# "heb", "ita", "por", "nob", "nno", "dan", "swe", "fra", "fin", "ell", "glg",
# "cat", "spa", "eus", "als", "pol", "slv")
vocabPortugues = set(w.lower() for w in nltk.corpus.wordnet.words('por'))
vocabIngles = set(w.lower() for w in nltk.corpus.wordnet.words('eng'))

# Função que separa a lista de aspectos simples retornando 3 listas (português, 
# inglês e a de palavras não reconheciadas pelos dicionários da base):
def obtemListasPortIng(lista):
  listaPortAux = []
  listaIngAux = []
  listaPalavrasNaoReconAux = []
  nlp = pt_core_news_sm.load()
  for termo in lista:
    w = []
    doc = nlp(termo)
    w = [token.lemma_ for token in doc]
    if ((w[0] in vocabPortugues) or (nltk.PorterStemmer().stem(termo)) in vocabPortugues) and (nltk.corpus.wordnet.morphy(termo) not in vocabIngles):
      listaPortAux.append(termo)
    else:
      if ((w[0] not in vocabPortugues) or (nltk.PorterStemmer().stem(termo)) not in vocabPortugues) and (nltk.corpus.wordnet.morphy(termo) not in vocabIngles):
        listaPalavrasNaoReconAux.append(termo)
      else:
        listaIngAux.append(termo)
  return listaPortAux, listaIngAux, listaPalavrasNaoReconAux

# Obtem-se 3 novas listas (português, inglês e a de termos não encontrados):
listaPortugues, listaIngles, listaPalavrasNaoRecon = obtemListasPortIng(listaAspectosSimples)

# Conferindo...
print(len(listaPortugues))
print(listaPortugues)

# Conferindo...
print(len(listaIngles))
print(listaIngles)

# Conferindo...
print(len(listaPalavrasNaoRecon))
print(listaPalavrasNaoRecon)

# Função que remove as stopwords da lista de palavras em português:
def removeStopWordsPort(lista):
    stopwords = nltk.corpus.stopwords.words('portuguese')
    listaAux = []
    listaAuxStopwords = []    
    for item in lista:
        if item not in stopwords:
            listaAux.append(item)
        else:
            listaAuxStopwords.append(item)
    return listaAux, listaAuxStopwords

# Função que remove as stopwords da lista de palavras em inglês:
def removeStopWordsIng(lista):
    stopwords = nltk.corpus.stopwords.words('english')
    listaAux = []
    listaAuxStopwords = []
    for item in lista:
        if item not in stopwords:
            listaAux.append(item)
        else:
            listaAuxStopwords.append(item)
    return listaAux, listaAuxStopwords

# Retira-se das listas de palavras em inglês e português as chamadas stopwords 
# que são palavras que não agregam muito valor para a análise do processamento 
# de linguagem natural (preposições, artigos, conjunções, por exemplo): 
listaPortugues, listaStopwordsPort = removeStopWordsPort(listaPortugues)
listaIngles, listaStopwordsIng = removeStopWordsIng(listaIngles)
# Pesquisa-se se na lista das "não encontradas" existe stopwords:
listaPalavrasNaoRecon, listaStopwordsNaoRecon = removeStopWordsPort(listaPalavrasNaoRecon)

# Conferindo...
print(len(listaPortugues))
print(listaPortugues)
print(listaStopwordsPort)

# Conferindo...
print(len(listaIngles))
print(listaIngles)
print(listaStopwordsIng)

# Conferindo...
print(len(listaPalavrasNaoRecon))
print(listaPalavrasNaoRecon)
print(listaStopwordsNaoRecon)

# Ao inspecionar a lista de palavras não reconhecidas nota-se que muitos termos 
# são utilizados no linguajar técnico da área de informática/tecnologia. 
# Sendo assim, vamos submeter a lista à um outro dicionário para extrair mais 
# palavras para a lista das palavras em inglês (a base "webtext" corresponde
# a coleção de textos retirados da web como do fórum de discussão da Firefox, 
# por exemplo): 
nltk.download('webtext')
vocabIngles2 = set(w for w in nltk.corpus.webtext.words())

# Rotina que recupera palavras da lista de não reconhecidas
listaPalavrasRecuperadas = []
listaPalavrasNaoReconFinal = []
for item in listaPalavrasNaoRecon:
  if item in vocabIngles2:
    listaPalavrasRecuperadas.append(item)
    listaIngles.append(item)
  else:
    listaPalavrasNaoReconFinal.append(item)

print(len(listaPalavrasRecuperadas))
print(listaPalavrasRecuperadas)

print(len(listaPalavrasNaoReconFinal))
print(listaPalavrasNaoReconFinal)

"""Agora que temos todas as listas separadas faremos o agrupamento dos termos semelhantes. Para a tarefa é empregada a propriedade "similarity" da biblioteca spaCy que avalia a similaridade semântica estimada entre as palavras."""

# Obtem os grupos de palavras semelhantes para as palavras em inglês:
nlp = en_core_web_lg.load()
listaInglesAux = [item for item in listaIngles]
listaInglesAux2 = [item for item in listaIngles]  
for item in listaInglesAux:
  grupoAux = []
  token1 = nlp(item)
    for item2 in listaInglesAux2:
      token2 = nlp(item2)
      if (token1.similarity(token2) > 0.6):
        grupoAux.append(item2)
  del listaInglesAux2[0]
  if (grupoAux):
    arquivo = open('/content/drive/My Drive/Grupos_palavras_ingles.txt', 'a')
    arquivo.write(str(grupoAux)) 
    arquivo.write('\n')
    arquivo.close()

# Obtem os grupos de palavras semelhantes para os termos compostos
# (aqueles termos que mantém mais de uma palavra no arquivo original)
nlp = en_core_web_lg.load()
listaAspectosCompostosAux = [item for item in listaAspectosCompostos]
listaAspectosCompostosAux2 = [item for item in listaAspectosCompostos]  
for item in listaAspectosCompostosAux:
  grupoAux = []
  token1 = nlp(item)
  for item2 in listaAspectosCompostosAux2:
    token2 = nlp(item2)
    if (token1.similarity(token2) > 0.8):
      grupoAux.append(item2)
  del listaAspectosCompostosAux2[0]
  if (grupoAux):
    arquivo = open('/content/drive/My Drive/Grupos_palavras_compostas.txt', 'a')
    arquivo.write(str(grupoAux)) 
    arquivo.write('\n')
    arquivo.close()

# Obtem os grupos de palavras semelhantes para as palavras "não reconhecidas"
# pelos dicionários
nlp = en_core_web_lg.load()
listaPalavrasNaoReconFinalAux = [item for item in listaPalavrasNaoReconFinal]
listaPalavrasNaoReconFinalAux2 = [item for item in listaPalavrasNaoReconFinal]  
for item in listaPalavrasNaoReconFinalAux:
  grupoAux = []
  token1 = nlp(item)
  for item2 in listaPalavrasNaoReconFinalAux2:
    token2 = nlp(item2)
    if (token1.similarity(token2) > 0.6):
      grupoAux.append(item2)
  del listaPalavrasNaoReconFinalAux2[0]
  if (grupoAux):
    arquivo = open('/content/drive/My Drive/Grupos_palavras_nao_encontradas.txt', 'a')
    arquivo.write(str(grupoAux)) 
    arquivo.write('\n')
    arquivo.close()

# Obtem os grupos de palavras semelhantes para as palavras em português:
nlp = pt_core_news_sm.load()
listaPortuguesAux = [item for item in listaPortugues]
listaPortuguesAux2 = [item for item in listaPortugues]
for item in listaPortuguesAux:
  grupoAux = []
  token1 = nlp(item)
  for item2 in listaPortuguesAux2:
    token2 = nlp(item2)
    if (token1.similarity(token2) > 0.8):
      grupoAux.append(item2)
  del listaPortuguesAux2[0]
  if (grupoAux):
    arquivo = open('/content/drive/My Drive/Grupos_palavras_portugues.txt', 'a')
    arquivo.write(str(grupoAux)) 
    arquivo.write('\n')
    arquivo.close()

"""De uma maneira geral os resultados mostram a obtenção de agrupamentos condizentes com a prposta. Uma abordagem interessante seria limpar os dados incoerentes (termos nitidamente antagônicos como as palavras antônimas, por exemplo, que são observados em alguns casos de grupos gerados). Outra ideia seria extrair todas as palavras óbvias (aquelas que apresentam o mesmo radical e portanto, possuem semântica muito parecida) restando somente aquelas que possuem significado parecido, mas uma escrita notoriamente distinta. 

Optou-se por não desmembrar os termos compostos da lista de entrada original (mais de uma palavra). A ideia foi preservar o forte significado que certos termos juntos podem apresentar. Apesar de alguns ajuntamentos um tanto estranhos (antônimos), os resultados mostram agrupamentos interessantes. Por exemplo, as expressões 'really excellent', 'quite good' e 'truly great' que são sinônimas praticamente e escritas muito diferentes foram colocadas num mesmo agrupamento.  

Os resultados para a lista de palavras em português não foram tão bons...a razão é que o modelo versão em português da biblioteca spaCy disponível para download não possui vetores incorporados até o momento, dea cordo com a documentação da ferramenta. Desse modo o resultado dos cálculos de similaridade do algoritmo fica prejudicado.

Com relação às palavras não identicadas a priori pelos dicionários utilizados os resultados demosntram que o algoritmo conseguiu agrupar termos semelhantes ou relacionados de alguma forma. Ex: 'usb', 'bluetooth', 'hdmi', 'vga', 'sata'
"""